{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b53f980",
   "metadata": {},
   "source": [
    "<div style=\"overflow:auto;\">\n",
    "    <div style=\"float:left; margin-right:10px;\">\n",
    "        <img width='120' height='120' src='https://cityme.novaims.unl.pt/images/footer/novaims.png'>\n",
    "    </div>\n",
    "\n",
    "# <p align=\"center\">Fidelidade chatbot</p>\n",
    "\n",
    "---\n",
    "\n",
    "## <p align=\"center\">*1 - Evaluation*</p>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we aim to evaluate the performance of a chatbot by comparing its responses to a set of reference question-answer (Q&A) pairs extracted from a PDF. The goal is to systematically assess how accurately and semantically appropriately the chatbot answers real-world questions, helping to identify areas for improvement and optimization.\n",
    "\n",
    "Our evaluation approach involves three key steps:\n",
    "\n",
    "1. **Q&A Extraction**:  \n",
    "   We parse a PDF document to extract structured Q&A pairs.\n",
    "\n",
    "2. **Chatbot Response Generation**:  \n",
    "   For each extracted question, we query the chatbot and record its generated answer along with the response time.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - **String Similarity**: We use fuzzy string matching (Levenshtein distance) to quantify the lexical similarity between the chatbot's answer and the expected answer.\n",
    "   - **Semantic Similarity**: We compute the cosine similarity between sentence embeddings (using a transformer model) to assess contextual alignment, enabling evaluation beyond surface-level word overlap.\n",
    "\n",
    "The output includes per-question evaluation results, summary statistics such as overall accuracy and average response time, and automatically saved reports for both string-based and semantic evaluations.\n",
    "\n",
    "This evaluation framework provides both quantitative insights and qualitative feedback on chatbot behavior, helping guide further training and refinement efforts.\n",
    "\n",
    "\n",
    "### üë• **Team Members**\n",
    "- **Ana Farinha** *(Student Number: 20211514)*  \n",
    "- **Ant√≥nio Oliveira** *(Student Number: 20211595)*  \n",
    "- **Mariana Neto** *(Student Number: 20211527)*  \n",
    "- **Salvador Domingues** *(Student Number: 20240597)*  \n",
    "\n",
    "üìÖ **Date:** *May 26, 2025*  \n",
    "üìç **Prepared for:** *FidelidadeC*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f068ab1",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Import Libraries & Data](#1-import-libraries--data)  \n",
    "2. [Evaluation](#2-evaluation)  \n",
    "   2.1. [String Similarity](#21-string-similarity)  \n",
    "   2.2. [Semantic Similarity](#22-semantic-similarity)\n",
    "   2.3. [Bot-based Similarity](#23-bot-based-similarity)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de664a",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856bdb56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:28:28.775198Z",
     "iopub.status.busy": "2025-05-26T19:28:28.774840Z",
     "iopub.status.idle": "2025-05-26T19:28:47.387973Z",
     "shell.execute_reply": "2025-05-26T19:28:47.387135Z",
     "shell.execute_reply.started": "2025-05-26T19:28:28.775168Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8J6pTdfaGgA5r193UVLsBshUspqwNpal42Jse1aHaok1cWNTLpRkJQQJ99BDACYeBjFXJ\n",
      "https://ai-bcds.openai.azure.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 20:28:38.325084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/bcwds4/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "2025-05-26 20:28:44.935 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:46.832 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:46.833 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:46.859 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.117 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/bcwds4/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-26 20:28:47.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.119 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-26 20:28:47.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 20:28:47.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from api import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from eval_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page\") \n",
    "warnings.filterwarnings(\"ignore\", message=\".*ScriptRunContext.*\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fabf3f",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc4f41",
   "metadata": {},
   "source": [
    "In this section, we extract question-and-answer (Q&A) pairs from a structured PDF document. These pairs will serve as the reference dataset for evaluating the performance of our chatbot.\n",
    "\n",
    "The extraction process is designed to:\n",
    "- Detect and separate questions (typically marked with a bullet point or other prefix),\n",
    "- Identify corresponding answers (often marked with a specific prefix like \"R:\"),\n",
    "- Clean and structure the text into a format suitable for further evaluation.\n",
    "\n",
    "This automated extraction ensures consistency, reduces manual effort, and prepares our data for downstream comparison using both string similarity and semantic evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1757d9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:28:50.302560Z",
     "iopub.status.busy": "2025-05-26T19:28:50.302202Z",
     "iopub.status.idle": "2025-05-26T19:28:50.723751Z",
     "shell.execute_reply": "2025-05-26T19:28:50.722999Z",
     "shell.execute_reply.started": "2025-05-26T19:28:50.302534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = extract_qa_pairs_from_pdf('./docs/Documents for training and evaluation-20250507/Questions_Answers_Censored.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249baa1",
   "metadata": {},
   "source": [
    "To understand how well our chatbot answers questions extracted from the PDF, we will evaluate its responses using several key metrics:\n",
    "\n",
    "| **Metric**            | **What It Measures**                                                                 |\n",
    "|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| **Accuracy**          | Percentage of chatbot answers that closely match the expected answer.               |\n",
    "| **Similarity Score**  | Fuzzy match score (0‚Äì100) comparing chatbot‚Äôs answer to the expected answer.        |\n",
    "| **Match (Yes/No)**    | Whether the answer is considered correct (based on a similarity threshold, e.g. 70).|\n",
    "| **Response Time**     | Time (in seconds) taken by the chatbot to respond to a question.                    |\n",
    "| **Average Response**  | Average of all response times ‚Äî reflects chatbot‚Äôs overall speed/efficiency.       |\n",
    "\n",
    "These metrics help quantify both the correctness and responsiveness of the chatbot, guiding improvements and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bfccb",
   "metadata": {},
   "source": [
    "# 2.1 String Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a4576",
   "metadata": {},
   "source": [
    "In this section, we assess how closely the chatbot's answers match the expected answers by measuring their textual similarity. We use fuzzy string matching techniques, which account for minor differences such as typos, paraphrasing, or formatting variations.\n",
    "\n",
    "The main tool used is the **FuzzyWuzzy** library, which calculates a similarity score between 0 and 100 based on the Levenshtein distance. Higher scores indicate greater similarity.\n",
    "\n",
    "By setting a similarity threshold, we classify chatbot answers as correct or incorrect. This method provides a straightforward, interpretable way to evaluate answer quality based purely on text overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6660f353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_multiple_evaluations(\n",
    "#     qa_pairs=qa_pairs,\n",
    "#     num_runs=15,\n",
    "#     threshold=70,\n",
    "#     evaluation_func=evaluate_string_and_save,\n",
    "#     summary_filename=\"string_summary_results\",\n",
    "#     detailed_filename=\"string_detailed_results\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50b300",
   "metadata": {},
   "source": [
    "#### Why String Similarity Evaluation Can Be Problematic\n",
    "\n",
    "Evaluating chatbot or assistant answers using **string similarity** methods can be unreliable for several reasons:\n",
    "\n",
    "- **Surface-Level Matching:** String similarity compares the literal characters or tokens in the text. It does not understand the *meaning* behind the answer. Two answers can be semantically equivalent but phrased very differently, leading to low similarity scores.\n",
    "\n",
    "- **Sensitive to Minor Differences:** Even small changes like punctuation, synonyms, or word order can drastically reduce the similarity score, unfairly marking correct answers as wrong.\n",
    "\n",
    "- **Ignores Context and Paraphrasing:** Good language models often rephrase answers or use synonyms, which string similarity methods might penalize despite the answer being correct.\n",
    "\n",
    "Because of these limitations, string similarity can lead to misleading evaluation results, especially in natural language tasks. Nonetheless, we include this section to show that we explored and considered this approach during our evaluation process.\n",
    "\n",
    "---\n",
    "\n",
    "#### Next Step: Evaluating with Semantic Similarity\n",
    "\n",
    "To better capture the *meaning* behind responses, we will move to **semantic similarity** evaluation. This approach uses embeddings from models like Sentence Transformers to compare the *contextual meaning* of two sentences, rather than just their surface forms.\n",
    "\n",
    "Semantic similarity can:\n",
    "\n",
    "- Recognize paraphrases and synonyms\n",
    "- Be more robust to minor wording differences\n",
    "- Provide a more accurate measure of whether the chatbot's answer truly matches the expected answer\n",
    "\n",
    "Next, we will implement and test semantic similarity evaluation using pre-trained sentence embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8cd6a",
   "metadata": {},
   "source": [
    "## 2.2 Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9d164",
   "metadata": {},
   "source": [
    "In this section, we assess chatbot answers by measuring the **similarity of their underlying meaning** rather than exact wording. Using **sentence embeddings** and **cosine similarity**, this approach provides a deeper understanding of response quality‚Äîespecially useful when answers are **rephrased** or use **different vocabulary**.\n",
    "\n",
    "We use models such as:\n",
    "- [`paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2): lightweight, fast, and multilingual, good for general-purpose evaluations.\n",
    "- [`LaBSE`](https://huggingface.co/sentence-transformers/LaBSE): a more powerful model designed for **language-agnostic sentence embeddings**, better suited for multilingual and semantically rich content.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Because semantic similarity is **less sensitive to wording variation** and tends to produce **stable results** (especially with low-generation-temperature settings), we typically perform **fewer evaluation runs**‚Äîoften a **single run** is sufficient.\n",
    "\n",
    "This type of evaluation offers a more **flexible and robust** way to judge chatbot performance, especially in real-world use cases where exact phrasing is less important than meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9260443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering the FAQ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:00<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary at threshold 0.5 =====\n",
      "Total: 17, Correct: 12, Accuracy: 70.59%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.5_1.csv\n",
      "\n",
      "===== Summary at threshold 0.55 =====\n",
      "Total: 17, Correct: 9, Accuracy: 52.94%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.55_1.csv\n",
      "\n",
      "===== Summary at threshold 0.6 =====\n",
      "Total: 17, Correct: 8, Accuracy: 47.06%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.6_1.csv\n",
      "\n",
      "===== Summary at threshold 0.65 =====\n",
      "Total: 17, Correct: 5, Accuracy: 29.41%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.65_1.csv\n",
      "\n",
      "===== Summary at threshold 0.7 =====\n",
      "Total: 17, Correct: 3, Accuracy: 17.65%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.7_1.csv\n",
      "\n",
      "===== Summary at threshold 0.75 =====\n",
      "Total: 17, Correct: 2, Accuracy: 11.76%, Avg Response Time: 3.46s\n",
      "Saved results to: evaluation\\paraphrase-multilingual-MiniLM-L12-v2_eval_results_threshold_0.75_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name_str = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name_str)\n",
    "\n",
    "# Get answers from chatbot\n",
    "chatbot_answers = answer_faq(qa_pairs, model, show_print=False)\n",
    "\n",
    "# Evaluate on different thresholds\n",
    "questions, results = evaluate_precomputed(chatbot_answers, model_name_str, thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a4717",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_evaluation_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_evaluation_metrics\u001b[49m(evaluations)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_evaluation_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_evaluation_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name_str = 'sentence-transformers/LaBSE'\n",
    "model = SentenceTransformer(model_name_str)\n",
    "\n",
    "# Get answers from chatbot\n",
    "chatbot_answers = answer_faq(qa_pairs, model, show_print=False)\n",
    "\n",
    "# Evaluate on different thresholds\n",
    "questions, results = evaluate_precomputed(chatbot_answers, model_name_str, thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fba4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494acd6",
   "metadata": {},
   "source": [
    "#### Comparison of Sentence Embedding Models for Portuguese Q&A Evaluation\n",
    "\n",
    "When evaluating question-answer pairs in Portuguese from Portugal (pt-PT), choosing the right sentence embedding model is crucial for balancing accuracy, speed, and language coverage. \n",
    "\n",
    "Below is a comparison between two popular models suitable for this task:\n",
    "\n",
    "| Feature / Model                                | paraphrase-multilingual-MiniLM-L12-v2         | LaBSE                                         |\n",
    "|------------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Publisher**                                  | Sentence-Transformers                         | Google Research                                |\n",
    "| **Languages Supported**                        | ~50+ (including pt-PT)                        | 100+ (strong pt-PT support)                   |\n",
    "| **Portuguese Variant**                         | Multilingual, supports both pt-PT and pt-BR   | Multilingual, very strong pt-PT support        |\n",
    "| **Model Size**                                 | ~135M parameters                              | ~470M parameters                              |\n",
    "| **Inference Speed**                            | ‚úÖ Fast                                        | ‚ùå Slower                                     |\n",
    "| **Accuracy (Semantic Similarity)**             | Moderate to High                              | High                                          |\n",
    "| **Fine-tuned For**                             | Paraphrase mining, sentence embeddings        | Language-agnostic sentence embeddings         |\n",
    "| **Training Dataset**                           | Multilingual paraphrase pairs                 | Translation and alignment-based corpora       |\n",
    "| **Sentence Embedding Quality**                 | Good, efficient                               | Very high, better alignment across languages  |\n",
    "| **Best Use Case**                              | Real-time evaluation, large-scale tests       | High-accuracy offline QA evaluation           |\n",
    "| **Model Hub**                                  | [`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | [`sentence-transformers/LaBSE`](https://huggingface.co/sentence-transformers/LaBSE) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_csv = \"evaluation/semantic_summary_results.csv\"\n",
    "detailed_csv = \"evaluation/semantic_detailed_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3b948-c013-4577-a3fb-1b633090bd8b",
   "metadata": {},
   "source": [
    "## 2.3 Bot-based Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57eaad-91d7-4c60-bfbb-dbb855c3f1b7",
   "metadata": {},
   "source": [
    "In this section, we evaluate the quality of answers generated by an assistant by comparing them to the expected answers using another LLM-based assistant as a judge. The judging assistant is prompted to assess the similarity between the expected and actual responses and return a numeric score from 0 to 10, with 10 indicating a perfect match.\n",
    "\n",
    "This bot-based evaluation method leverages the language model's ability to interpret semantic meaning, making it useful for assessing open-ended or context-rich answers where traditional string matching techniques may fall short. By automating this process, we can efficiently and consistently evaluate large sets of question-answer pairs.\n",
    "\n",
    "As the assistant‚Äôs evaluations are generative in nature, slight variability in scores is expected between runs, even when the inputs remain the same. This is a natural consequence of the probabilistic nature of language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4b3702-0361-465e-8b1b-ac41841b3449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:28:55.266811Z",
     "iopub.status.busy": "2025-05-26T19:28:55.266472Z",
     "iopub.status.idle": "2025-05-26T19:30:35.339314Z",
     "shell.execute_reply": "2025-05-26T19:30:35.338189Z",
     "shell.execute_reply.started": "2025-05-26T19:28:55.266784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Q1...\n",
      "Evaluating Q2...\n",
      "Evaluating Q3...\n",
      "Evaluating Q4...\n",
      "Evaluating Q5...\n",
      "Evaluating Q6...\n",
      "Evaluating Q7...\n",
      "Evaluating Q8...\n",
      "Evaluating Q9...\n",
      "Evaluating Q10...\n",
      "Evaluating Q11...\n",
      "Evaluating Q12...\n",
      "Evaluating Q13...\n",
      "Evaluating Q14...\n",
      "Evaluating Q15...\n",
      "Evaluating Q16...\n",
      "Evaluating Q17...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for idx, (question, expected_answer) in enumerate(qa_pairs):\n",
    "    print(f\"Evaluating Q{idx+1}...\")\n",
    "    \n",
    "    # Get assistant's answer\n",
    "    actual_answer = query_assistant(question)\n",
    "    evaluation = eval_chat(question, expected_answer, actual_answer)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"evaluation\": evaluation\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84eb9ba6-207a-44c1-99d4-bad76717efee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:31:16.944109Z",
     "iopub.status.busy": "2025-05-26T19:31:16.943513Z",
     "iopub.status.idle": "2025-05-26T19:31:16.948225Z",
     "shell.execute_reply": "2025-05-26T19:31:16.947445Z",
     "shell.execute_reply.started": "2025-05-26T19:31:16.944083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 6\n",
      "Q2 8\n",
      "Q3 0\n",
      "Q4 8\n",
      "Q5 8\n",
      "Q6 7\n",
      "Q7 7\n",
      "Q8 7\n",
      "Q9 7\n",
      "Q10 7\n",
      "Q11 8\n",
      "Q12 2\n",
      "Q13 4\n",
      "Q14 3\n",
      "Q15 8\n",
      "Q16 6\n",
      "Q17 8\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    print(f'Q{i+1}', results[i][\"evaluation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775124c1-3ae9-4ac1-b2c7-f374ac091805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:31:32.964315Z",
     "iopub.status.busy": "2025-05-26T19:31:32.963968Z",
     "iopub.status.idle": "2025-05-26T19:31:32.969071Z",
     "shell.execute_reply": "2025-05-26T19:31:32.968343Z",
     "shell.execute_reply.started": "2025-05-26T19:31:32.964287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce0fc3-eaf3-4ebd-a562-a741c9f35c3b",
   "metadata": {},
   "source": [
    "**Save Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8b61ec-a759-4ac8-ad6f-0759bf8b9460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:32:24.110309Z",
     "iopub.status.busy": "2025-05-26T19:32:24.109890Z",
     "iopub.status.idle": "2025-05-26T19:32:24.247208Z",
     "shell.execute_reply": "2025-05-26T19:32:24.246591Z",
     "shell.execute_reply.started": "2025-05-26T19:32:24.110277Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_excel('./evaluation/bot_eval.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
