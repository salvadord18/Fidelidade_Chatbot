{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b53f980",
   "metadata": {},
   "source": [
    "<div style=\"overflow:auto;\">\n",
    "    <div style=\"float:left; margin-right:10px;\">\n",
    "        <img width='120' height='120' src='https://cityme.novaims.unl.pt/images/footer/novaims.png'>\n",
    "    </div>\n",
    "\n",
    "# <p align=\"center\">Fidelidade chatbot</p>\n",
    "\n",
    "---\n",
    "\n",
    "## <p align=\"center\">*1 - Evaluation*</p>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we aim to evaluate the performance of a chatbot by comparing its responses to a set of reference question-answer (Q&A) pairs extracted from a PDF. The goal is to systematically assess how accurately and semantically appropriately the chatbot answers real-world questions, helping to identify areas for improvement and optimization.\n",
    "\n",
    "Our evaluation approach involves three key steps:\n",
    "\n",
    "1. **Q&A Extraction**:  \n",
    "   We parse a PDF document to extract structured Q&A pairs.\n",
    "\n",
    "2. **Chatbot Response Generation**:  \n",
    "   For each extracted question, we query the chatbot and record its generated answer along with the response time.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - **String Similarity**: We use fuzzy string matching (Levenshtein distance) to quantify the lexical similarity between the chatbot's answer and the expected answer.\n",
    "   - **Semantic Similarity**: We compute the cosine similarity between sentence embeddings (using a transformer model) to assess contextual alignment, enabling evaluation beyond surface-level word overlap.\n",
    "\n",
    "The output includes per-question evaluation results, summary statistics such as overall accuracy and average response time, and automatically saved reports for both string-based and semantic evaluations.\n",
    "\n",
    "This evaluation framework provides both quantitative insights and qualitative feedback on chatbot behavior, helping guide further training and refinement efforts.\n",
    "\n",
    "\n",
    "### üë• **Team Members**\n",
    "- **Ana Farinha** *(Student Number: 20211514)*  \n",
    "- **Ant√≥nio Oliveira** *(Student Number: 20211595)*  \n",
    "- **Mariana Neto** *(Student Number: 20211527)*  \n",
    "- **Salvador Domingues** *(Student Number: 20240597)*  \n",
    "\n",
    "üìÖ **Date:** *May 26, 2025*  \n",
    "üìç **Prepared for:** *FidelidadeC*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f068ab1",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Import Libraries & Data](#1-import-libraries--data)  \n",
    "2. [Evaluation](#2-evaluation)  \n",
    "   2.1. [String Similarity](#21-string-similarity)  \n",
    "   2.2. [Semantic Similarity](#22-semantic-similarity)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de664a",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856bdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8J6pTdfaGgA5r193UVLsBshUspqwNpal42Jse1aHaok1cWNTLpRkJQQJ99BDACYeBjFXJ\n",
      "'https://ai-bcds.openai.azure.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdar\\anaconda3\\envs\\Case4BCwDS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-24 17:46:04.025 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.128 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.128 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.164 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.889 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\abdar\\anaconda3\\envs\\Case4BCwDS\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-24 17:46:07.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.893 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.895 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-24 17:46:07.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.903 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.906 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.917 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.923 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.927 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.931 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.934 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.941 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.945 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.997 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:07.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:08.003 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:08.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:08.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-24 17:46:08.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from api import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from eval_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page\") \n",
    "warnings.filterwarnings(\"ignore\", message=\".*ScriptRunContext.*\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fabf3f",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc4f41",
   "metadata": {},
   "source": [
    "In this section, we extract question-and-answer (Q&A) pairs from a structured PDF document. These pairs will serve as the reference dataset for evaluating the performance of our chatbot.\n",
    "\n",
    "The extraction process is designed to:\n",
    "- Detect and separate questions (typically marked with a bullet point or other prefix),\n",
    "- Identify corresponding answers (often marked with a specific prefix like \"R:\"),\n",
    "- Clean and structure the text into a format suitable for further evaluation.\n",
    "\n",
    "This automated extraction ensures consistency, reduces manual effort, and prepares our data for downstream comparison using both string similarity and semantic evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1757d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = extract_qa_pairs_from_pdf('./docs/Documents for training and evaluation-20250507\\Questions_Answers_Censored.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249baa1",
   "metadata": {},
   "source": [
    "To understand how well our chatbot answers questions extracted from the PDF, we will evaluate its responses using several key metrics:\n",
    "\n",
    "| **Metric**            | **What It Measures**                                                                 |\n",
    "|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| **Accuracy**          | Percentage of chatbot answers that closely match the expected answer.               |\n",
    "| **Similarity Score**  | Fuzzy match score (0‚Äì100) comparing chatbot‚Äôs answer to the expected answer.        |\n",
    "| **Match (Yes/No)**    | Whether the answer is considered correct (based on a similarity threshold, e.g. 70).|\n",
    "| **Response Time**     | Time (in seconds) taken by the chatbot to respond to a question.                    |\n",
    "| **Average Response**  | Average of all response times ‚Äî reflects chatbot‚Äôs overall speed/efficiency.       |\n",
    "\n",
    "These metrics help quantify both the correctness and responsiveness of the chatbot, guiding improvements and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bfccb",
   "metadata": {},
   "source": [
    "# 2.1 String Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a4576",
   "metadata": {},
   "source": [
    "In this section, we assess how closely the chatbot's answers match the expected answers by measuring their textual similarity. We use fuzzy string matching techniques, which account for minor differences such as typos, paraphrasing, or formatting variations.\n",
    "\n",
    "The main tool used is the **FuzzyWuzzy** library, which calculates a similarity score between 0 and 100 based on the Levenshtein distance. Higher scores indicate greater similarity.\n",
    "\n",
    "By setting a similarity threshold, we classify chatbot answers as correct or incorrect. This method provides a straightforward, interpretable way to evaluate answer quality based purely on text overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6660f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:00<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.54s\n",
      "\n",
      "### Running Evaluation 2/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:53<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.16s\n",
      "\n",
      "### Running Evaluation 3/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:07<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.96s\n",
      "\n",
      "### Running Evaluation 4/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:56<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.35s\n",
      "\n",
      "### Running Evaluation 5/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:55<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.25s\n",
      "\n",
      "### Running Evaluation 6/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:55<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.27s\n",
      "\n",
      "### Running Evaluation 7/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.44s\n",
      "\n",
      "### Running Evaluation 8/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:02<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.70s\n",
      "\n",
      "### Running Evaluation 9/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:00<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.57s\n",
      "\n",
      "### Running Evaluation 10/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:57<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.39s\n",
      "\n",
      "### Running Evaluation 11/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 12/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 13/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 14/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:56<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.34s\n",
      "\n",
      "### Running Evaluation 15/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:59<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.50s\n",
      "\n",
      "Saved detailed results to: evaluation\\string_detailed_results.csv\n",
      "Saved summary results to: evaluation\\string_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=15,\n",
    "    threshold=70,\n",
    "    evaluation_func=evaluate_string_and_save,\n",
    "    summary_filename=\"string_summary_results\",\n",
    "    detailed_filename=\"string_detailed_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50b300",
   "metadata": {},
   "source": [
    "#### Why String Similarity Evaluation Can Be Problematic\n",
    "\n",
    "Evaluating chatbot or assistant answers using **string similarity** methods can be unreliable for several reasons:\n",
    "\n",
    "- **Surface-Level Matching:** String similarity compares the literal characters or tokens in the text. It does not understand the *meaning* behind the answer. Two answers can be semantically equivalent but phrased very differently, leading to low similarity scores.\n",
    "\n",
    "- **Sensitive to Minor Differences:** Even small changes like punctuation, synonyms, or word order can drastically reduce the similarity score, unfairly marking correct answers as wrong.\n",
    "\n",
    "- **Ignores Context and Paraphrasing:** Good language models often rephrase answers or use synonyms, which string similarity methods might penalize despite the answer being correct.\n",
    "\n",
    "Because of these limitations, string similarity can produce misleading evaluation results, especially in more complex or natural language scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### Next Step: Evaluating with Semantic Similarity\n",
    "\n",
    "To better capture the *meaning* behind responses, we will move to **semantic similarity** evaluation. This approach uses embeddings from models like Sentence Transformers to compare the *contextual meaning* of two sentences, rather than just their surface forms.\n",
    "\n",
    "Semantic similarity can:\n",
    "\n",
    "- Recognize paraphrases and synonyms\n",
    "- Be more robust to minor wording differences\n",
    "- Provide a more accurate measure of whether the chatbot's answer truly matches the expected answer\n",
    "\n",
    "Next, we will implement and test semantic similarity evaluation using pre-trained sentence embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8cd6a",
   "metadata": {},
   "source": [
    "## 2.2 Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9d164",
   "metadata": {},
   "source": [
    "In this section, we assess chatbot answers by measuring the **similarity of their underlying meaning** rather than exact wording. Using **sentence embeddings** and **cosine similarity**, this approach provides a deeper understanding of response quality‚Äîespecially useful when answers are **rephrased** or use **different vocabulary**.\n",
    "\n",
    "We use models such as:\n",
    "- [`paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2): lightweight, fast, and multilingual, good for general-purpose evaluations.\n",
    "- [`LaBSE`](https://huggingface.co/sentence-transformers/LaBSE): a more powerful model designed for **language-agnostic sentence embeddings**, better suited for multilingual and semantically rich content.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Because semantic similarity is **less sensitive to wording variation** and tends to produce **stable results** (especially with low-generation-temperature settings), we typically perform **fewer evaluation runs**‚Äîoften a **single run** is sufficient.\n",
    "\n",
    "This type of evaluation offers a more **flexible and robust** way to judge chatbot performance, especially in real-world use cases where exact phrasing is less important than meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa318b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/1 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:06<00:00,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 3, Accuracy: 17.65%, Avg Response Time: 3.55s\n",
      "\n",
      "Saved detailed results to: evaluation\\semantic_detailed_results.csv\n",
      "Saved summary results to: evaluation\\semantic_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=1,\n",
    "    threshold=0.7,\n",
    "    model=model,\n",
    "    evaluation_func=evaluate_semantic_and_save,\n",
    "    summary_filename=\"semantic_summary_results\",\n",
    "    detailed_filename=\"semantic_detailed_results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891b3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/1 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:17<00:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 2, Accuracy: 11.76%, Avg Response Time: 3.80s\n",
      "\n",
      "Saved detailed results to: evaluation\\semantic_detailed_results.csv\n",
      "Saved summary results to: evaluation\\semantic_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=1,\n",
    "    threshold=0.7,\n",
    "    model=model,\n",
    "    evaluation_func=evaluate_semantic_and_save,\n",
    "    summary_filename=\"semantic_summary_results\",\n",
    "    detailed_filename=\"semantic_detailed_results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494acd6",
   "metadata": {},
   "source": [
    "#### Comparison of Sentence Embedding Models for Portuguese Q&A Evaluation\n",
    "\n",
    "When evaluating question-answer pairs in Portuguese from Portugal (pt-PT), choosing the right sentence embedding model is crucial for balancing accuracy, speed, and language coverage. \n",
    "\n",
    "Below is a comparison between two popular models suitable for this task:\n",
    "\n",
    "| Feature / Model                                | paraphrase-multilingual-MiniLM-L12-v2         | LaBSE                                         |\n",
    "|------------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Publisher**                                  | Sentence-Transformers                         | Google Research                                |\n",
    "| **Languages Supported**                        | ~50+ (including pt-PT)                        | 100+ (strong pt-PT support)                   |\n",
    "| **Portuguese Variant**                         | Multilingual, supports both pt-PT and pt-BR   | Multilingual, very strong pt-PT support        |\n",
    "| **Model Size**                                 | ~135M parameters                              | ~470M parameters                              |\n",
    "| **Inference Speed**                            | ‚úÖ Fast                                        | ‚ùå Slower                                     |\n",
    "| **Accuracy (Semantic Similarity)**             | Moderate to High                              | High                                          |\n",
    "| **Fine-tuned For**                             | Paraphrase mining, sentence embeddings        | Language-agnostic sentence embeddings         |\n",
    "| **Training Dataset**                           | Multilingual paraphrase pairs                 | Translation and alignment-based corpora       |\n",
    "| **Sentence Embedding Quality**                 | Good, efficient                               | Very high, better alignment across languages  |\n",
    "| **Best Use Case**                              | Real-time evaluation, large-scale tests       | High-accuracy offline QA evaluation           |\n",
    "| **Model Hub**                                  | [`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | [`sentence-transformers/LaBSE`](https://huggingface.co/sentence-transformers/LaBSE) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved in folder: plots\n"
     ]
    }
   ],
   "source": [
    "summary_csv = \"evaluation/semantic_summary_results.csv\"\n",
    "detailed_csv = \"evaluation/semantic_detailed_results.csv\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
