{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b53f980",
   "metadata": {},
   "source": [
    "<div style=\"overflow:auto;\">\n",
    "    <div style=\"float:left; margin-right:10px;\">\n",
    "        <img width='120' height='120' src='https://cityme.novaims.unl.pt/images/footer/novaims.png'>\n",
    "    </div>\n",
    "\n",
    "# <p align=\"center\">Fidelidade chatbot</p>\n",
    "\n",
    "---\n",
    "\n",
    "## <p align=\"center\">*1 - Evaluation*</p>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we aim to evaluate the performance of a chatbot by comparing its responses to a set of reference question-answer (Q&A) pairs extracted from a PDF. The goal is to systematically assess how accurately and semantically appropriately the chatbot answers real-world questions, helping to identify areas for improvement and optimization.\n",
    "\n",
    "Our evaluation approach involves three key steps:\n",
    "\n",
    "1. **Q&A Extraction**:  \n",
    "   We parse a PDF document to extract structured Q&A pairs.\n",
    "\n",
    "2. **Chatbot Response Generation**:  \n",
    "   For each extracted question, we query the chatbot and record its generated answer along with the response time.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - **String Similarity**: We use fuzzy string matching (Levenshtein distance) to quantify the lexical similarity between the chatbot's answer and the expected answer.\n",
    "   - **Semantic Similarity**: We compute the cosine similarity between sentence embeddings (using a transformer model) to assess contextual alignment, enabling evaluation beyond surface-level word overlap.\n",
    "\n",
    "The output includes per-question evaluation results, summary statistics such as overall accuracy and average response time, and automatically saved reports for both string-based and semantic evaluations.\n",
    "\n",
    "This evaluation framework provides both quantitative insights and qualitative feedback on chatbot behavior, helping guide further training and refinement efforts.\n",
    "\n",
    "\n",
    "### üë• **Team Members**\n",
    "- **Ana Farinha** *(Student Number: 20211514)*  \n",
    "- **Ant√≥nio Oliveira** *(Student Number: 20211595)*  \n",
    "- **Mariana Neto** *(Student Number: 20211527)*  \n",
    "- **Salvador Domingues** *(Student Number: 20240597)*  \n",
    "\n",
    "üìÖ **Date:** *May 26, 2025*  \n",
    "üìç **Prepared for:** *FidelidadeC*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f068ab1",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Import Libraries & Data](#1-import-libraries--data)  \n",
    "2. [Evaluation](#2-evaluation)  \n",
    "   2.1. [String Similarity](#21-string-similarity)  \n",
    "   2.2. [Semantic Similarity](#22-semantic-similarity)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de664a",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856bdb56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T13:34:07.018014Z",
     "iopub.status.busy": "2025-05-26T13:34:07.017654Z",
     "iopub.status.idle": "2025-05-26T13:34:21.710625Z",
     "shell.execute_reply": "2025-05-26T13:34:21.709722Z",
     "shell.execute_reply.started": "2025-05-26T13:34:07.017978Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8J6pTdfaGgA5r193UVLsBshUspqwNpal42Jse1aHaok1cWNTLpRkJQQJ99BDACYeBjFXJ\n",
      "https://ai-bcds.openai.azure.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 14:34:13.822911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/bcwds4/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "2025-05-26 14:34:19.616 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.244 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.245 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.269 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.460 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/bcwds4/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-26 14:34:21.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.462 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-26 14:34:21.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.467 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.467 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.470 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.470 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.471 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.471 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.490 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-26 14:34:21.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from api import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from eval_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page\") \n",
    "warnings.filterwarnings(\"ignore\", message=\".*ScriptRunContext.*\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fabf3f",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc4f41",
   "metadata": {},
   "source": [
    "In this section, we extract question-and-answer (Q&A) pairs from a structured PDF document. These pairs will serve as the reference dataset for evaluating the performance of our chatbot.\n",
    "\n",
    "The extraction process is designed to:\n",
    "- Detect and separate questions (typically marked with a bullet point or other prefix),\n",
    "- Identify corresponding answers (often marked with a specific prefix like \"R:\"),\n",
    "- Clean and structure the text into a format suitable for further evaluation.\n",
    "\n",
    "This automated extraction ensures consistency, reduces manual effort, and prepares our data for downstream comparison using both string similarity and semantic evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1757d9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T13:34:27.438157Z",
     "iopub.status.busy": "2025-05-26T13:34:27.437519Z",
     "iopub.status.idle": "2025-05-26T13:34:27.807235Z",
     "shell.execute_reply": "2025-05-26T13:34:27.805633Z",
     "shell.execute_reply.started": "2025-05-26T13:34:27.438130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = extract_qa_pairs_from_pdf('./docs/Documents for training and evaluation-20250507/Questions_Answers_Censored.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249baa1",
   "metadata": {},
   "source": [
    "To understand how well our chatbot answers questions extracted from the PDF, we will evaluate its responses using several key metrics:\n",
    "\n",
    "| **Metric**            | **What It Measures**                                                                 |\n",
    "|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| **Accuracy**          | Percentage of chatbot answers that closely match the expected answer.               |\n",
    "| **Similarity Score**  | Fuzzy match score (0‚Äì100) comparing chatbot‚Äôs answer to the expected answer.        |\n",
    "| **Match (Yes/No)**    | Whether the answer is considered correct (based on a similarity threshold, e.g. 70).|\n",
    "| **Response Time**     | Time (in seconds) taken by the chatbot to respond to a question.                    |\n",
    "| **Average Response**  | Average of all response times ‚Äî reflects chatbot‚Äôs overall speed/efficiency.       |\n",
    "\n",
    "These metrics help quantify both the correctness and responsiveness of the chatbot, guiding improvements and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bfccb",
   "metadata": {},
   "source": [
    "# 2.1 String Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a4576",
   "metadata": {},
   "source": [
    "In this section, we assess how closely the chatbot's answers match the expected answers by measuring their textual similarity. We use fuzzy string matching techniques, which account for minor differences such as typos, paraphrasing, or formatting variations.\n",
    "\n",
    "The main tool used is the **FuzzyWuzzy** library, which calculates a similarity score between 0 and 100 based on the Levenshtein distance. Higher scores indicate greater similarity.\n",
    "\n",
    "By setting a similarity threshold, we classify chatbot answers as correct or incorrect. This method provides a straightforward, interpretable way to evaluate answer quality based purely on text overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6660f353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:00<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.54s\n",
      "\n",
      "### Running Evaluation 2/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:53<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.16s\n",
      "\n",
      "### Running Evaluation 3/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:07<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.96s\n",
      "\n",
      "### Running Evaluation 4/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:56<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.35s\n",
      "\n",
      "### Running Evaluation 5/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:55<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.25s\n",
      "\n",
      "### Running Evaluation 6/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:55<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.27s\n",
      "\n",
      "### Running Evaluation 7/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.44s\n",
      "\n",
      "### Running Evaluation 8/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:02<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.70s\n",
      "\n",
      "### Running Evaluation 9/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:00<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.57s\n",
      "\n",
      "### Running Evaluation 10/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:57<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.39s\n",
      "\n",
      "### Running Evaluation 11/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 12/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 13/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:58<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.45s\n",
      "\n",
      "### Running Evaluation 14/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:56<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.34s\n",
      "\n",
      "### Running Evaluation 15/15 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:59<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 0, Accuracy: 0.00%, Avg Response Time: 3.50s\n",
      "\n",
      "Saved detailed results to: evaluation\\string_detailed_results.csv\n",
      "Saved summary results to: evaluation\\string_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=15,\n",
    "    threshold=70,\n",
    "    evaluation_func=evaluate_string_and_save,\n",
    "    summary_filename=\"string_summary_results\",\n",
    "    detailed_filename=\"string_detailed_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50b300",
   "metadata": {},
   "source": [
    "#### Why String Similarity Evaluation Can Be Problematic\n",
    "\n",
    "Evaluating chatbot or assistant answers using **string similarity** methods can be unreliable for several reasons:\n",
    "\n",
    "- **Surface-Level Matching:** String similarity compares the literal characters or tokens in the text. It does not understand the *meaning* behind the answer. Two answers can be semantically equivalent but phrased very differently, leading to low similarity scores.\n",
    "\n",
    "- **Sensitive to Minor Differences:** Even small changes like punctuation, synonyms, or word order can drastically reduce the similarity score, unfairly marking correct answers as wrong.\n",
    "\n",
    "- **Ignores Context and Paraphrasing:** Good language models often rephrase answers or use synonyms, which string similarity methods might penalize despite the answer being correct.\n",
    "\n",
    "Because of these limitations, string similarity can produce misleading evaluation results, especially in more complex or natural language scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### Next Step: Evaluating with Semantic Similarity\n",
    "\n",
    "To better capture the *meaning* behind responses, we will move to **semantic similarity** evaluation. This approach uses embeddings from models like Sentence Transformers to compare the *contextual meaning* of two sentences, rather than just their surface forms.\n",
    "\n",
    "Semantic similarity can:\n",
    "\n",
    "- Recognize paraphrases and synonyms\n",
    "- Be more robust to minor wording differences\n",
    "- Provide a more accurate measure of whether the chatbot's answer truly matches the expected answer\n",
    "\n",
    "Next, we will implement and test semantic similarity evaluation using pre-trained sentence embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8cd6a",
   "metadata": {},
   "source": [
    "## 2.2 Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9d164",
   "metadata": {},
   "source": [
    "In this section, we assess chatbot answers by measuring the **similarity of their underlying meaning** rather than exact wording. Using **sentence embeddings** and **cosine similarity**, this approach provides a deeper understanding of response quality‚Äîespecially useful when answers are **rephrased** or use **different vocabulary**.\n",
    "\n",
    "We use models such as:\n",
    "- [`paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2): lightweight, fast, and multilingual, good for general-purpose evaluations.\n",
    "- [`LaBSE`](https://huggingface.co/sentence-transformers/LaBSE): a more powerful model designed for **language-agnostic sentence embeddings**, better suited for multilingual and semantically rich content.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Because semantic similarity is **less sensitive to wording variation** and tends to produce **stable results** (especially with low-generation-temperature settings), we typically perform **fewer evaluation runs**‚Äîoften a **single run** is sufficient.\n",
    "\n",
    "This type of evaluation offers a more **flexible and robust** way to judge chatbot performance, especially in real-world use cases where exact phrasing is less important than meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa318b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/1 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:06<00:00,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 3, Accuracy: 17.65%, Avg Response Time: 3.55s\n",
      "\n",
      "Saved detailed results to: evaluation\\semantic_detailed_results.csv\n",
      "Saved summary results to: evaluation\\semantic_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=1,\n",
    "    threshold=0.7,\n",
    "    model=model,\n",
    "    evaluation_func=evaluate_semantic_and_save,\n",
    "    summary_filename=\"semantic_summary_results\",\n",
    "    detailed_filename=\"semantic_detailed_results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891b3fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Evaluation 1/1 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:17<00:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary =====\n",
      "Total: 17, Correct: 2, Accuracy: 11.76%, Avg Response Time: 3.80s\n",
      "\n",
      "Saved detailed results to: evaluation\\semantic_detailed_results.csv\n",
      "Saved summary results to: evaluation\\semantic_summary_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "run_multiple_evaluations(\n",
    "    qa_pairs=qa_pairs,\n",
    "    num_runs=1,\n",
    "    threshold=0.7,\n",
    "    model=model,\n",
    "    evaluation_func=evaluate_semantic_and_save,\n",
    "    summary_filename=\"semantic_summary_results\",\n",
    "    detailed_filename=\"semantic_detailed_results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494acd6",
   "metadata": {},
   "source": [
    "#### Comparison of Sentence Embedding Models for Portuguese Q&A Evaluation\n",
    "\n",
    "When evaluating question-answer pairs in Portuguese from Portugal (pt-PT), choosing the right sentence embedding model is crucial for balancing accuracy, speed, and language coverage. \n",
    "\n",
    "Below is a comparison between two popular models suitable for this task:\n",
    "\n",
    "| Feature / Model                                | paraphrase-multilingual-MiniLM-L12-v2         | LaBSE                                         |\n",
    "|------------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Publisher**                                  | Sentence-Transformers                         | Google Research                                |\n",
    "| **Languages Supported**                        | ~50+ (including pt-PT)                        | 100+ (strong pt-PT support)                   |\n",
    "| **Portuguese Variant**                         | Multilingual, supports both pt-PT and pt-BR   | Multilingual, very strong pt-PT support        |\n",
    "| **Model Size**                                 | ~135M parameters                              | ~470M parameters                              |\n",
    "| **Inference Speed**                            | ‚úÖ Fast                                        | ‚ùå Slower                                     |\n",
    "| **Accuracy (Semantic Similarity)**             | Moderate to High                              | High                                          |\n",
    "| **Fine-tuned For**                             | Paraphrase mining, sentence embeddings        | Language-agnostic sentence embeddings         |\n",
    "| **Training Dataset**                           | Multilingual paraphrase pairs                 | Translation and alignment-based corpora       |\n",
    "| **Sentence Embedding Quality**                 | Good, efficient                               | Very high, better alignment across languages  |\n",
    "| **Best Use Case**                              | Real-time evaluation, large-scale tests       | High-accuracy offline QA evaluation           |\n",
    "| **Model Hub**                                  | [`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | [`sentence-transformers/LaBSE`](https://huggingface.co/sentence-transformers/LaBSE) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved in folder: plots\n"
     ]
    }
   ],
   "source": [
    "summary_csv = \"evaluation/semantic_summary_results.csv\"\n",
    "detailed_csv = \"evaluation/semantic_detailed_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3b948-c013-4577-a3fb-1b633090bd8b",
   "metadata": {},
   "source": [
    "## 2.2 Bot-based Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57eaad-91d7-4c60-bfbb-dbb855c3f1b7",
   "metadata": {},
   "source": [
    "In this section, we evaluate the quality of answers generated by an assistant by comparing them to the expected answers using another LLM-based assistant as a judge. The judging assistant is prompted to assess the similarity between the expected and actual responses and return a numeric score from 0 to 10, with 10 indicating a perfect match.\n",
    "\n",
    "This bot-based evaluation method leverages the language model's ability to interpret semantic meaning, making it useful for assessing open-ended or context-rich answers where traditional string matching techniques may fall short. By automating this process, we can efficiently and consistently evaluate large sets of question-answer pairs.\n",
    "\n",
    "As the assistant‚Äôs evaluations are generative in nature, slight variability in scores is expected between runs, even when the inputs remain the same. This is a natural consequence of the probabilistic nature of language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4b3702-0361-465e-8b1b-ac41841b3449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T13:37:07.239649Z",
     "iopub.status.busy": "2025-05-26T13:37:07.239302Z",
     "iopub.status.idle": "2025-05-26T13:38:59.604571Z",
     "shell.execute_reply": "2025-05-26T13:38:59.603356Z",
     "shell.execute_reply.started": "2025-05-26T13:37:07.239623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Q1...\n",
      "Evaluating Q2...\n",
      "Evaluating Q3...\n",
      "Evaluating Q4...\n",
      "Evaluating Q5...\n",
      "Evaluating Q6...\n",
      "Evaluating Q7...\n",
      "Evaluating Q8...\n",
      "Evaluating Q9...\n",
      "Evaluating Q10...\n",
      "Evaluating Q11...\n",
      "Evaluating Q12...\n",
      "Evaluating Q13...\n",
      "Evaluating Q14...\n",
      "Evaluating Q15...\n",
      "Evaluating Q16...\n",
      "Evaluating Q17...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for idx, (question, expected_answer) in enumerate(qa_pairs):\n",
    "    print(f\"Evaluating Q{idx+1}...\")\n",
    "    \n",
    "    # Get assistant's answer\n",
    "    actual_answer = query_assistant(question)\n",
    "    evaluation = eval_chat(question, expected_answer, actual_answer)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"evaluation\": evaluation\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84eb9ba6-207a-44c1-99d4-bad76717efee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T13:38:59.606660Z",
     "iopub.status.busy": "2025-05-26T13:38:59.606179Z",
     "iopub.status.idle": "2025-05-26T13:38:59.610915Z",
     "shell.execute_reply": "2025-05-26T13:38:59.610121Z",
     "shell.execute_reply.started": "2025-05-26T13:38:59.606628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 5\n",
      "Q2 8\n",
      "Q3 7\n",
      "Q4 7\n",
      "Q5 7\n",
      "Q6 7\n",
      "Q7 7\n",
      "Q8 8\n",
      "Q9 7\n",
      "Q10 4\n",
      "Q11 4\n",
      "Q12 2\n",
      "Q13 7\n",
      "Q14 7\n",
      "Q15 8\n",
      "Q16 6\n",
      "Q17 8\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    print(f'Q{i+1}', results[i][\"evaluation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce0fc3-eaf3-4ebd-a562-a741c9f35c3b",
   "metadata": {},
   "source": [
    "**Save Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b61ec-a759-4ac8-ad6f-0759bf8b9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_eval = results.to_csv('./evaluation/bot_eval.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
